###### 实时数仓

###### Flink CDC

​		Flink CDC Source 并行度为1，因为 Flink 不能保证 Source 算子不同分区的数据顺序，并发处理将导致数据混乱，但是 Sink 时并行度可以不为 1，此时需要在中间进行 key by 主键，保证主键对应数据的顺序。

踩坑：

* flink-cdc-connectors 依赖了kafka-connect 2.5.0，所以自己的项目中依赖的 kafka 和 kafka-connector 版本也需要是2.5.0，否则将出现如下报错

  > Caused by: java.lang.AbstractMethodError: Method org/apache/kafka/connect/json/JsonSerializer.configure(Ljava/util/Map;Z)V is abstract

* flink 1.8后不再继承 hadoop 包，推荐自行下载对应 jar 包，而从1.10 开始推荐用户设置环境变量以使 flink 可以找到 hadoop 相关 class，可以直接在shell中键入以下命令或直接添加到 `~/.bashrc`中，若 source 后仍未生效可重启系统 

  ```
  `export HADOOP_CLASSPATH=`hadoop classpath`
  ```
  
    [flink 集成 hadoop](https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/ops/deployment/hadoop.html)，未集成而直接在 job 中使用 hadoop 功能则出现如下报错
  
  > UnsupportedFileSystemSchemeException: Hadoop is not in the classpath/dependencies.

